---
title: Insert title here
key: 972fe35ea4e257689c1ab53913553465

---
## Introduction to Logistic Regression

```yaml
type: "TitleSlide"
key: "618541c966"
```

`@lower_third`

name: Michael Babawale
title: Instructor 


`@script`
In this lesson, I will explain the reasoning behind Logistic Regression. Also, we will discuss the meaning of a decision boundary, and how it applies to logistic regression.


---
## **__Reasoning behind Logistic Regression__**

```yaml
type: "FullSlide"
key: "d549d6df65"
```

`@part1`
- **Logistic Regression is useful for classification** 

- **In logistic regression, the response or target variable _(y)_ is qualitative**

- **In addition, logistic regression models are probabilistic i.e., the model predicts the probability that _y_ belongs to a particular class.**

- **Logistic Regression is a linear classifier, i.e, the model creates a 'linear decision boundary' (will be explained in next slide)**


`@script`
Logistic Regression models fall under the classification section of supervised machine learning. Thus, for logistic regression, the target variable is of a qualitative type. However, the features can be either quantitative or qualitative, but that is not the focus of our discussion for now.

An important thing to note about a Logistic Regression model is that it does not model the target variable directly, rather, it models the probability that the target variable belongs to a particular class or category


---
## **Linear Decision Boundary**

```yaml
type: "FullImageSlide"
key: "c190fe71f8"
```

`@part1`
![](https://assets.datacamp.com/production/repositories/4326/datasets/22b324eb119b37faf250fcad1158dc3ad3723c85/decisionboundary.png)


`@script`
let's explain how a logistic model performs classification. Imagine having a straight line separating different classes/categories, such that all the points in the data fall on either side of the line, as seen in the picture. This line is called the 'decision boundary'. In the image, the decision boundary was created by a Logistic Regression model. It is able to separate the datapoints into two qualitative categories, 'A', and 'B'. In effect, when you fit a logistic model to data, what you are basically doing is fitting a linear decision boundary to separate the data into two or more categories. This example shown is for two categories; however, the linear decision boundary can be extended to three or more categories. 

The observant learner would notice that some points on the image are misclassified; in further lessons, we will be able to compute the number of misclassified points, and other interesting metrics which tell us about the accuracy of our Logistic Regression model.


---
## Class probability

```yaml
type: "FullCodeSlide"
key: "11a6d88e35"
```

`@part1`
``


`@script`



---
## Let's practice

```yaml
type: "FinalSlide"
key: "cd8a045234"
```

`@script`
Now, let's practice the key concepts we talked about in this lesson

